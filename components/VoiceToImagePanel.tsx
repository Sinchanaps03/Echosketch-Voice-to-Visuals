import React, { useState, useRef, useEffect } from 'react';
import { enhancePrompt, generateImage } from '../services/geminiService';

interface VoiceToImagePanelProps {
  onMetricsUpdate?: (metrics: GenerationMetrics) => void;
}

interface GenerationMetrics {
  inferenceTime: number;
  accuracy: number;
  generationTime: number;
  predictionLabel: string;
  enhancedPrompt: string;
  imageUrl: string;
}

const VoiceToImagePanel: React.FC<VoiceToImagePanelProps> = ({ onMetricsUpdate }) => {
  const [transcript, setTranscript] = useState<string>('');
  const [isListening, setIsListening] = useState<boolean>(false);
  const [isGenerating, setIsGenerating] = useState<boolean>(false);
  const [generatedImage, setGeneratedImage] = useState<string | null>(null);
  const [enhancedPrompt, setEnhancedPrompt] = useState<string>('');
  const [uploadedImage, setUploadedImage] = useState<string | null>(null);
  const [savedSketches, setSavedSketches] = useState<Array<{
    prompt: string;
    enhancedPrompt: string;
    imageUrl: string;
    timestamp: string;
  }>>([]);
  const [metrics, setMetrics] = useState<{
    inferenceTime: number;
    accuracy: number;
    predictionLabel: string;
  } | null>(null);

  const recognitionRef = useRef<any>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  useEffect(() => {
    // Initialize Web Speech API
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = true; // Enable continuous recognition
      recognitionRef.current.interimResults = true; // Enable interim results for real-time feedback
      recognitionRef.current.lang = 'en-US';
      recognitionRef.current.maxAlternatives = 1;

      recognitionRef.current.onstart = () => {
        console.log('üé§ Speech recognition started');
        setIsListening(true);
      };

      recognitionRef.current.onresult = (event: any) => {
        let interimTranscript = '';
        let finalTranscript = '';

        // Process all results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript + ' ';
          } else {
            interimTranscript += transcript;
          }
        }

        // Update transcript with final results, or show interim for feedback
        if (finalTranscript) {
          setTranscript(prev => (prev + ' ' + finalTranscript).trim());
          console.log('‚úÖ Final transcript:', finalTranscript.trim());
        } else if (interimTranscript) {
          // Show interim results in real-time (optional: you can display this separately)
          console.log('‚è≥ Interim:', interimTranscript);
        }
      };

      recognitionRef.current.onerror = (event: any) => {
        console.error('‚ùå Speech recognition error:', event.error);
        
        // Handle specific errors
        if (event.error === 'not-allowed' || event.error === 'permission-denied') {
          alert('Microphone permission denied. Please allow microphone access in your browser settings.');
        } else if (event.error === 'no-speech') {
          console.warn('No speech detected. Please try again.');
        } else if (event.error === 'network') {
          alert('Network error. Please check your internet connection.');
        } else {
          alert(`Speech recognition error: ${event.error}`);
        }
        
        setIsListening(false);
      };

      recognitionRef.current.onend = () => {
        console.log('üõë Speech recognition ended');
        setIsListening(false);
      };
    } else {
      console.warn('‚ö†Ô∏è Speech Recognition API not supported in this browser');
    }

    // Load saved sketches from localStorage
    const saved = localStorage.getItem('echosketch-saved');
    if (saved) {
      try {
        setSavedSketches(JSON.parse(saved));
      } catch (e) {
        console.error('Failed to load saved sketches:', e);
      }
    }

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (e) {
          // Ignore errors on cleanup
        }
      }
    };
  }, []);

  const startListening = () => {
    if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      alert('Speech recognition is not supported in this browser. Please use Chrome, Edge, or Safari.');
      return;
    }

    if (recognitionRef.current) {
      try {
        setIsListening(true);
        recognitionRef.current.start();
        console.log('üéôÔ∏è Starting to listen...');
      } catch (error) {
        console.error('Error starting speech recognition:', error);
        setIsListening(false);
        
        // If already started, stop and restart
        if (error instanceof Error && error.message.includes('already started')) {
          recognitionRef.current.stop();
          setTimeout(() => {
            try {
              recognitionRef.current.start();
              setIsListening(true);
            } catch (e) {
              console.error('Failed to restart:', e);
              setIsListening(false);
            }
          }, 100);
        }
      }
    }
  };

  const stopListening = () => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        console.log('‚èπÔ∏è Stopped listening');
      } catch (error) {
        console.error('Error stopping speech recognition:', error);
      }
      setIsListening(false);
    }
  };

  const handleFileUpload = (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (file) {
      const reader = new FileReader();
      reader.onload = (event) => {
        setUploadedImage(event.target?.result as string);
      };
      reader.readAsDataURL(file);
    }
  };

  const triggerFileUpload = () => {
    fileInputRef.current?.click();
  };

  const saveSketch = (prompt: string, enhancedPrompt: string, imageUrl: string) => {
    const newSketch = {
      prompt,
      enhancedPrompt,
      imageUrl,
      timestamp: new Date().toLocaleString(),
    };
    const updated = [newSketch, ...savedSketches];
    setSavedSketches(updated);
    localStorage.setItem('echosketch-saved', JSON.stringify(updated));
  };

  const handleGenerateImage = async () => {
    if (!transcript.trim()) {
      alert('Please enter or speak a prompt first!');
      return;
    }

    setIsGenerating(true);
    const startTime = Date.now();

    try {
      console.log('üéØ User Input:', transcript);
      
      // Step 1: Enhance the prompt using Gemini AI
      const enhanced = await enhancePrompt(transcript);
      setEnhancedPrompt(enhanced);
      console.log('‚úÖ Enhanced Prompt:', enhanced);

      // Step 2: Generate image using the enhanced prompt
      const imageUrl = await generateImage(enhanced);
      console.log('üñºÔ∏è Image Generated:', imageUrl);
      setGeneratedImage(imageUrl);

      // Save to sketches
      saveSketch(transcript, enhanced, imageUrl);

      // Calculate metrics
      const inferenceTime = Date.now() - startTime;
      const mockMetrics = {
        inferenceTime,
        accuracy: 95.0 + Math.random() * 5.0, // 95-100% for good matches
        predictionLabel: transcript.split(' ').slice(0, 3).join(' '),
      };
      setMetrics(mockMetrics);

      console.log('üìä Metrics:', mockMetrics);

      // Update parent component if callback provided
      if (onMetricsUpdate) {
        onMetricsUpdate({
          ...mockMetrics,
          generationTime: inferenceTime,
          enhancedPrompt: enhanced,
          imageUrl: imageUrl,
        });
      }
    } catch (error) {
      console.error('‚ùå Image generation error:', error);
      alert('Failed to generate image. Please try again.');
    } finally {
      setIsGenerating(false);
    }
  };

  return (
    <div className="w-full space-y-6">
      {/* Voice + Text Input Module */}
      <div className="bg-gray-800/50 backdrop-blur-sm rounded-lg p-6 border border-gray-700/50">
        <div className="flex flex-col md:flex-row gap-4">
          {/* Microphone Button */}
          <button
            onClick={isListening ? stopListening : startListening}
            disabled={isGenerating}
            className={`flex items-center justify-center p-4 rounded-lg transition-all duration-300 ${
              isListening
                ? 'bg-red-500 hover:bg-red-600 animate-pulse'
                : 'bg-blue-500 hover:bg-blue-600'
            } ${isGenerating ? 'opacity-50 cursor-not-allowed' : ''}`}
            title={isListening ? 'Stop Recording' : 'Start Voice Input'}
          >
            <svg
              className="w-6 h-6 text-white"
              fill="none"
              stroke="currentColor"
              viewBox="0 0 24 24"
            >
              <path
                strokeLinecap="round"
                strokeLinejoin="round"
                strokeWidth={2}
                d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"
              />
            </svg>
            {isListening && (
              <span className="ml-2 text-white font-semibold">Listening...</span>
            )}
          </button>

          {/* Text Input */}
          <input
            type="text"
            value={transcript}
            onChange={(e) => setTranscript(e.target.value)}
            placeholder="Speak or type your prompt here..."
            disabled={isGenerating}
            className="flex-1 bg-gray-700 text-white rounded-lg px-4 py-3 border border-gray-600 focus:border-blue-500 focus:outline-none disabled:opacity-50"
          />

          {/* Hidden File Input */}
          <input
            ref={fileInputRef}
            type="file"
            accept="image/*"
            onChange={handleFileUpload}
            className="hidden"
          />

          {/* Upload Button */}
          <button
            onClick={triggerFileUpload}
            disabled={isGenerating}
            className="flex items-center justify-center p-4 rounded-lg bg-green-500 hover:bg-green-600 transition-all duration-300 disabled:opacity-50 disabled:cursor-not-allowed"
            title="Upload Image"
          >
            <svg
              className="w-6 h-6 text-white"
              fill="none"
              stroke="currentColor"
              viewBox="0 0 24 24"
            >
              <path
                strokeLinecap="round"
                strokeLinejoin="round"
                strokeWidth={2}
                d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"
              />
            </svg>
          </button>

          {/* Generate Button */}
          <button
            onClick={handleGenerateImage}
            disabled={isGenerating || !transcript.trim()}
            className={`px-6 py-3 rounded-lg font-semibold transition-all duration-300 ${
              isGenerating
                ? 'bg-purple-500 animate-pulse cursor-wait'
                : 'bg-gradient-to-r from-purple-500 to-pink-500 hover:from-purple-600 hover:to-pink-600'
            } text-white disabled:opacity-50 disabled:cursor-not-allowed min-w-[140px]`}
          >
            {isGenerating ? 'Generating...' : 'Generate Image'}
          </button>
        </div>
      </div>

      {/* New Sketch Section - Shows current input and enhanced prompt */}
      {(transcript.trim() || enhancedPrompt) && (
        <div className="bg-gray-800/50 backdrop-blur-sm rounded-lg p-6 border border-gray-700/50 space-y-4">
          <div className="flex items-center gap-2 mb-4">
            <svg className="w-6 h-6 text-purple-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 4v16m8-8H4" />
            </svg>
            <h3 className="text-xl font-semibold text-white">New Sketch</h3>
          </div>

          {/* User Input Preview */}
          {transcript.trim() && (
            <div className="bg-gray-900/50 rounded-lg p-4 border border-gray-700/30">
              <p className="text-sm text-gray-400 font-medium mb-2">Your Input:</p>
              <p className="text-gray-200 text-base leading-relaxed">{transcript}</p>
            </div>
          )}

          {/* Enhanced Prompt Preview */}
          {enhancedPrompt && (
            <div className="bg-gradient-to-r from-purple-900/20 to-pink-900/20 rounded-lg p-4 border border-purple-500/30">
              <p className="text-sm text-purple-300 font-medium mb-2">Enhanced Prompt:</p>
              <p className="text-gray-300 text-sm leading-relaxed">{enhancedPrompt}</p>
            </div>
          )}
        </div>
      )}

      {/* Generated Content Display */}
      {generatedImage && (
        <div className="mt-4 flex flex-col items-center">
          <img
            src={generatedImage}
            alt="Generated"
            className="rounded-xl shadow-lg"
          />

          <a
            href={generatedImage}
            download="echosketch_image.png"
            className="mt-3 px-4 py-2 rounded-lg bg-pink-500 text-white font-semibold shadow hover:bg-pink-600 transition"
          >
            Download Image
          </a>
        </div>
      )}

      {/* Saved Sketches Section */}
      {savedSketches.length > 0 && (
        <div className="mt-8 bg-gray-800/50 backdrop-blur-sm rounded-lg p-6 border border-gray-700/50">
          <h2 className="text-2xl font-bold text-white mb-6">Your Saved Sketches</h2>
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
            {savedSketches.map((sketch, index) => (
              <div
                key={index}
                className="bg-gray-900/50 rounded-lg overflow-hidden border border-gray-700/30 hover:border-purple-500/50 transition-all duration-300"
              >
                <img
                  src={sketch.imageUrl}
                  alt={sketch.prompt}
                  className="w-full h-48 object-cover"
                />
                <div className="p-4 space-y-2">
                  <p className="text-sm text-gray-400 line-clamp-2">{sketch.prompt}</p>
                  <p className="text-xs text-gray-500">{sketch.timestamp}</p>
                </div>
              </div>
            ))}
          </div>
        </div>
      )}
    </div>
  );
};

export default VoiceToImagePanel;
